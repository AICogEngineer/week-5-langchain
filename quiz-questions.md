# Weekly Knowledge Check: LangChain v1.0

## Part 1: Multiple Choice

### 1. What is the correct import path for the `@tool` decorator in LangChain v1.0?
- [ ] A) `from langchain.tools import tool`
- [ ] B) `from langchain_core.tools import tool`
- [ ] C) `from langchain.agents import tool`
- [ ] D) `from langchain.decorators import tool`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) `from langchain_core.tools import tool`

**Explanation:** In LangChain v1.0, the `@tool` decorator is imported from `langchain_core.tools`. This is the standardized location for core tool functionality.
- **Why others are wrong:**
  - A) `langchain.tools` is not the correct module path
  - C) `langchain.agents` contains agent creation functions, not the tool decorator
  - D) `langchain.decorators` does not exist
</details>

---

### 2. Which environment variable must be set to `true` to enable LangSmith tracing?
- [ ] A) `LANGSMITH_PROJECT`
- [ ] B) `LANGSMITH_API_KEY`
- [ ] C) `LANGSMITH_ENDPOINT`
- [ ] D) `LANGSMITH_TRACING`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** D) `LANGSMITH_TRACING`

**Explanation:** You must set `LANGSMITH_TRACING=true` to enable tracing. This tells LangChain to send trace data to LangSmith.
- **Why others are wrong:**
  - A) `LANGSMITH_PROJECT` is optional (defaults to "default")
  - B) `LANGSMITH_API_KEY` is required but is set to your key value, not "true"
  - C) `LANGSMITH_ENDPOINT` is optional and defaults to the production endpoint
</details>

---

### 3. When creating a tool with the `@tool` decorator, what is the PRIMARY purpose of the docstring?
- [ ] A) To satisfy Python syntax requirements
- [ ] B) To generate API documentation
- [ ] C) To help the agent understand when and how to use the tool
- [ ] D) To define the tool's return type

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) To help the agent understand when and how to use the tool

**Explanation:** The docstring is the primary way agents understand your tool. A descriptive docstring tells the agent what the tool does and when to use it, enabling proper routing of user requests to the appropriate tool.
- **Why others are wrong:**
  - A) While Python syntax allows empty docstrings, this defeats the purpose
  - B) API documentation is a secondary benefit
  - D) Return type is specified via type hints, not the docstring
</details>

---

### 4. What is the correct provider string format for using Claude via AWS Bedrock with `init_chat_model()`?
- [ ] A) `claude:bedrock:anthropic.claude-3-5-sonnet`
- [ ] B) `bedrock:anthropic.claude-3-5-sonnet-20241022-v2:0`
- [ ] C) `aws:claude-3-5-sonnet`
- [ ] D) `anthropic:bedrock:claude-3-5-sonnet`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) `bedrock:anthropic.claude-3-5-sonnet-20241022-v2:0`

**Explanation:** The provider string format is `provider:model_id`. For AWS Bedrock, the provider is `bedrock` and the model ID includes the full ARN-style identifier like `anthropic.claude-3-5-sonnet-20241022-v2:0`.
- **Why others are wrong:**
  - A) Uses incorrect double-colon format and incomplete model ID
  - C) `aws` is not a valid provider string
  - D) Uses incorrect format with double provider specification
</details>

---

### 5. Which checkpointer is appropriate for production deployments?
- [ ] A) `InMemorySaver`
- [ ] B) `SessionMemory`
- [ ] C) `SqliteSaver` or `PostgresSaver`
- [ ] D) `ConversationBufferMemory`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) `SqliteSaver` or `PostgresSaver`

**Explanation:** For production, database-backed checkpointers like `SqliteSaver` or `PostgresSaver` provide persistence across restarts. `InMemorySaver` is only for development because data is lost when the process ends.
- **Why others are wrong:**
  - A) `InMemorySaver` is volatile and loses data on restart
  - B) `SessionMemory` is not a LangChain v1.0 checkpointer class
  - D) `ConversationBufferMemory` is a deprecated v0.x pattern
</details>

---

### 6. What parameter must be passed in the config to identify a conversation when using checkpointers?
- [ ] A) `session_id`
- [ ] B) `conversation_id`
- [ ] C) `memory_key`
- [ ] D) `thread_id`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** D) `thread_id`

**Explanation:** The `thread_id` in the config (e.g., `{"configurable": {"thread_id": "session_123"}}`) identifies which conversation's memory/state to use. Different thread IDs maintain separate, isolated conversations.
- **Why others are wrong:**
  - A) `session_id` is not recognized by LangChain checkpointers
  - B) `conversation_id` is not the correct parameter name
  - C) `memory_key` is not used in LangChain v1.0
</details>

---

### 7. Which method is used to attach a Pydantic schema for structured output from a model?
- [ ] A) `.parse_output(PydanticModel)`
- [ ] B) `.set_schema(PydanticModel)`
- [ ] C) `.format_response(PydanticModel)`
- [ ] D) `.with_structured_output(PydanticModel)`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** D) `.with_structured_output(PydanticModel)`

**Explanation:** The `.with_structured_output()` method attaches a Pydantic schema to either a model or an agent, ensuring the output conforms to the specified structure with automatic validation.
- **Why others are wrong:**
  - A) `.parse_output()` is not a LangChain method
  - B) `.set_schema()` is not a LangChain method
  - C) `.format_response()` is not a LangChain method
</details>

---

### 8. What is the v1.0 recommended approach for creating a simple chat agent?
- [ ] A) Build a custom LangGraph StateGraph with nodes
- [ ] B) Use `create_react_agent()` from `langchain.agents`
- [ ] C) Use `create_agent()` from `langchain.agents`
- [ ] D) Chain prompts using LCEL pipe operators

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) Use `create_agent()` from `langchain.agents`

**Explanation:** LangChain v1.0 emphasizes "simplicity first" with the `create_agent()` helper function. It handles tool calling, state management, and message history automaticallyâ€”suitable for 90% of agent needs.
- **Why others are wrong:**
  - A) Manual LangGraph is only for complex custom workflows
  - B) `create_react_agent()` is deprecated in v1.0
  - D) LCEL has been deprecated and removed in v1.0
</details>

---

### 9. Which method returns results from a model one token at a time?
- [ ] A) `.invoke()`
- [ ] B) `.batch()`
- [ ] C) `.stream()`
- [ ] D) `.process()`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) `.stream()`

**Explanation:** The `.stream()` method returns an iterator that yields tokens or chunks as they're generated, enabling real-time streaming output. This is useful for chat interfaces where you want to display responses as they're generated.
- **Why others are wrong:**
  - A) `.invoke()` waits for the complete response
  - B) `.batch()` processes multiple inputs at once but returns complete responses
  - D) `.process()` is not a LangChain model method
</details>

---

### 10. What is the recommended return type for tools to ensure best agent compatibility?
- [ ] A) `int`
- [ ] B) `dict`
- [ ] C) `str`
- [ ] D) `bool`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) `str`

**Explanation:** Tools should always return strings for best agent compatibility. String returns are easily incorporated into the agent's conversation flow and can include formatted details that agents interpret well.
- **Why others are wrong:**
  - A) Integer returns may not be interpreted correctly by all agents
  - B) Dictionaries require additional parsing
  - D) Boolean returns don't provide enough context for agents
</details>

---

## Part 2: True/False

### 11. LCEL (LangChain Expression Language) pipe operators like `prompt | model | parser` are the recommended approach in LangChain v1.0.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** False

**Explanation:** LCEL has been deprecated and removed in LangChain v1.0. The v1.0 approach emphasizes using helper functions like `create_agent()` and direct method calls (`.invoke()`, `.stream()`) instead of chain composition with pipe operators.
</details>

---

### 12. When using `InMemorySaver`, conversation state persists even after the Python process restarts.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** False

**Explanation:** `InMemorySaver` stores state in RAM, which is volatile. All data is lost when the process ends. For persistence across restarts, use database-backed checkpointers like `SqliteSaver` or `PostgresSaver`.
</details>

---

### 13. The `name` parameter is optional when calling `create_agent()`.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** False

**Explanation:** While technically not required by the function signature, the `name` parameter should ALWAYS be provided. Names are essential for debugging, tracing in LangSmith, and identifying agents in logsâ€”especially critical in multi-agent systems.
</details>

---

### 14. LangSmith tracing requires code changes to your agent implementation.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** False

**Explanation:** LangSmith tracing is enabled purely through environment variables (`LANGSMITH_TRACING=true`, `LANGSMITH_API_KEY`). No code changes are neededâ€”LangChain automatically traces all operations when these variables are set.
</details>

---

### 15. The `init_chat_model()` helper function automatically handles API key authentication from environment variables.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** True

**Explanation:** `init_chat_model()` automatically reads API keys from standard environment variables like `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, or AWS credentials for Bedrock. This is part of its "simplicity first" design.
</details>

---

### 16. In a multi-agent supervisor pattern, sub-agents communicate directly with users.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** False

**Explanation:** In the supervisor (tool-calling) pattern, sub-agents do NOT talk to users directly. They return results to the supervisor agent, which then communicates with the user. The supervisor maintains centralized control over user interaction.
</details>

---

### 17. Pydantic's `with_structured_output()` automatically validates that the LLM's response matches the schema.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** True

**Explanation:** When you use `.with_structured_output(MyPydanticModel)`, Pydantic automatically validates the LLM's output against your schema. If the output doesn't match, a `ValidationError` is raised.
</details>

---

### 18. The same checkpointer instance can be shared across multiple agents.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** True

**Explanation:** A single checkpointer can serve multiple agents. Each agent's conversations remain isolated through different `thread_id` values, but they all use the same storage backend.
</details>

---

## Part 3: Code Prediction

### 19. What is the output of this code?
```python
from langchain import init_chat_model

model_string = "openai:gpt-4o-mini"
parts = model_string.split(":")
print(f"Provider: {parts[0]}, Model: {parts[1]}")
```
- [ ] A) Provider: openai, Model: gpt-4o-mini
- [ ] B) Provider: gpt-4o-mini, Model: openai
- [ ] C) Error: invalid split
- [ ] D) Provider: openai:gpt-4o-mini, Model: None

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** A) Provider: openai, Model: gpt-4o-mini

**Explanation:** The string `"openai:gpt-4o-mini"` split by `:` produces `["openai", "gpt-4o-mini"]`. `parts[0]` is `"openai"` and `parts[1]` is `"gpt-4o-mini"`.
</details>

---

### 20. What happens when this code runs?
```python
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[],
    checkpointer=InMemorySaver(),
    name="test_agent"
)

config = {"configurable": {"thread_id": "session_a"}}
agent.invoke({"messages": [{"role": "user", "content": "My name is Alice"}]}, config)

config2 = {"configurable": {"thread_id": "session_b"}}
result = agent.invoke({"messages": [{"role": "user", "content": "What is my name?"}]}, config2)
```
- [ ] A) The agent knows the name is Alice
- [ ] B) The agent does not know the name (different thread_id)
- [ ] C) A runtime error occurs due to missing thread_id
- [ ] D) Both sessions share the same memory

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) The agent does not know the name (different thread_id)

**Explanation:** Each `thread_id` maintains a separate conversation context. Since `session_b` is different from `session_a`, the agent has no memory of the name "Alice" being mentionedâ€”that information is isolated to `session_a`.
</details>

---

### 21. What will this code print?
```python
from pydantic import BaseModel, Field
from typing import Literal

class Sentiment(BaseModel):
    label: Literal["positive", "negative", "neutral"]
    score: float

s = Sentiment(label="positive", score=0.95)
print(type(s.label).__name__)
```
- [ ] A) Literal
- [ ] B) str
- [ ] C) Sentiment
- [ ] D) label

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) str

**Explanation:** While the type hint is `Literal["positive", "negative", "neutral"]`, the actual value stored is a regular Python string. The `Literal` type hint constrains which values are valid but doesn't change the runtime typeâ€”`"positive"` is just a `str`.
</details>

---

## Part 4: Fill-in-the-Blank

### 22. To enable LangSmith tracing, you must set the environment variable `LANGSMITH_TRACING` to `_____`.

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** `true`

**Explanation:** The exact value must be `true` (lowercase, as a string). Values like `"True"`, `"1"`, or `"yes"` will not work.
</details>

---

### 23. The import path for `InMemorySaver` is `from langgraph.checkpoint._____ import InMemorySaver`.

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** `memory`

**Explanation:** The full import is `from langgraph.checkpoint.memory import InMemorySaver`. The `memory` module contains the in-memory checkpointer implementation.
</details>

---

### 24. When using a checkpointer, you must pass a `_____` in the config to identify the conversation.

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** `thread_id`

**Explanation:** The config dictionary's `configurable` key should contain a `thread_id` that uniquely identifies the conversation. Example: `{"configurable": {"thread_id": "user_session_123"}}`.
</details>

---

### 25. The function `model._____structured_output(PydanticModel)` attaches a Pydantic schema to a model.

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** `with_`

**Explanation:** The method is `model.with_structured_output(PydanticModel)`. It returns a new model wrapper that ensures output conforms to the specified Pydantic schema.
</details>

---

## Part 5: Multiple Choice (Continued)

### 26. What is the recommended chunk size for `RecursiveCharacterTextSplitter` in typical RAG applications?
- [ ] A) 100-200 characters
- [ ] B) 500-800 characters
- [ ] C) 800-1200 characters
- [ ] D) 5000+ characters

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) 800-1200 characters

**Explanation:** A typical configuration is `chunk_size=1000` with `chunk_overlap=200`. Chunks too small (<500) lose context, while chunks too large (>2000) reduce precision in retrieval.
- **Why others are wrong:**
  - A) 100-200 is too small and loses context
  - B) 500-800 is on the lower end, less typical
  - D) 5000+ is too large for effective retrieval
</details>

---

### 27. Which lifecycle hook runs AFTER tools have completed execution?
- [ ] A) `@before_model`
- [ ] B) `@before_tools`
- [ ] C) `@dynamic_prompt`
- [ ] D) `@after_tools`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** D) `@after_tools`

**Explanation:** The `@after_tools` hook runs after tool execution completes. It's useful for validating tool results, logging, or modifying state based on what tools returned.
- **Why others are wrong:**
  - A) `@before_model` runs before the LLM call
  - B) `@before_tools` runs before tools execute
  - C) `@dynamic_prompt` modifies the system prompt, not tool lifecycle
</details>

---

### 28. Which of the following is a deprecated pattern in LangChain v1.0?
- [ ] A) `create_agent()` from `langchain.agents`
- [ ] B) `@tool` decorator from `langchain_core.tools`
- [ ] C) `ConversationBufferMemory` from `langchain.memory`
- [ ] D) `InMemorySaver` from `langgraph.checkpoint.memory`

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) `ConversationBufferMemory` from `langchain.memory`

**Explanation:** `ConversationBufferMemory` is a legacy v0.x pattern. In v1.0, memory is handled through checkpointers like `InMemorySaver` which integrate with the new agent architecture.
- **Why others are wrong:**
  - A) `create_agent()` is the v1.0 standard
  - B) `@tool` is the v1.0 standard for tool creation
  - D) `InMemorySaver` is the v1.0 approach to checkpointing
</details>

---

### 29. In the supervisor multi-agent pattern, what determines which sub-agent the supervisor calls?
- [ ] A) The sub-agent's `name` parameter
- [ ] B) The tool description wrapping the sub-agent
- [ ] C) A random selection algorithm
- [ ] D) The order agents were added to the tools list

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) The tool description wrapping the sub-agent

**Explanation:** In the supervisor pattern, sub-agents are wrapped as tools. The supervisor uses the tool's description to decide which sub-agent to call for a given requestâ€”making well-written tool descriptions critical.
- **Why others are wrong:**
  - A) The agent name helps with debugging but doesn't drive routing
  - C) Routing is deterministic based on understanding, not random
  - D) Tool order doesn't affect the LLM's routing decisions
</details>

---

### 30. What is the PRIMARY benefit of using `init_chat_model()` over provider-specific classes?
- [ ] A) Better performance and lower latency
- [ ] B) Access to more model parameters
- [ ] C) Simplified provider switching with a single string format
- [ ] D) Automatic retry logic built-in

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** C) Simplified provider switching with a single string format

**Explanation:** `init_chat_model()` lets you switch providers by changing a string (e.g., `"openai:gpt-4o-mini"` to `"anthropic:claude-3-haiku"`) instead of changing imports and class names. This is the core value proposition.
- **Why others are wrong:**
  - A) Performance is the same; it creates the same underlying objects
  - B) Provider-specific classes actually offer MORE parameters
  - D) Retry logic is configured separately via `max_retries`
</details>

---

### 31. What should a sub-agent's system prompt emphasize in a supervisor pattern?
- [ ] A) Instructions for handling user conversation
- [ ] B) Including all results in the final message
- [ ] C) Calling other sub-agents when needed
- [ ] D) Managing its own conversation memory

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) Including all results in the final message

**Explanation:** A common failure mode is sub-agents doing work but not including results in their final message. The supervisor only sees the sub-agent's final output, so prompts should emphasize: "Include ALL results in your final response."
- **Why others are wrong:**
  - A) Sub-agents don't talk to users directly
  - C) Sub-agents don't call other sub-agents in the basic pattern
  - D) Memory is managed by the supervisor, not sub-agents
</details>

---

### 32. Which configuration method allows you to change what tools are available to an agent based on runtime conditions?
- [ ] A) `@before_model` middleware
- [ ] B) `@wrap_model_call` middleware
- [ ] C) `@after_tools` middleware
- [ ] D) `@dynamic_prompt` middleware

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) `@wrap_model_call` middleware

**Explanation:** `@wrap_model_call` can intercept the model request and modify it, including changing the tools list with `request.override(tools=...)`. This enables dynamic tool selection based on user permissions or other conditions.
- **Why others are wrong:**
  - A) `@before_model` modifies state, not model call parameters
  - C) `@after_tools` runs after tools execute, too late to change them
  - D) `@dynamic_prompt` only modifies the system prompt
</details>

---

### 33. What is the similarity score interpretation for a "good" match in LangChain vector stores?
- [ ] A) Score > 2.0
- [ ] B) Score between 0.5 and 1.0
- [ ] C) Score between 5.0 and 10.0
- [ ] D) Score < 0.1

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) Score between 0.5 and 1.0

**Explanation:** In LangChain vector stores, lower scores mean more similar. Scores < 0.5 are excellent, 0.5-1.0 are good, and > 1.5 are questionable. This is distance-based scoring.
- **Why others are wrong:**
  - A) Score > 2.0 indicates poor/distant matches
  - C) Scores in 5.0-10.0 range would be very poor matches
  - D) While < 0.1 is excellent, the question asked about "good" (0.5-1.0)
</details>

---

### 34. What happens when you call `agent.invoke()` without passing a config when the agent has a checkpointer?
- [ ] A) The agent works normally without memory
- [ ] B) An error is raised for missing thread_id
- [ ] C) The agent uses a default thread_id
- [ ] D) The checkpointer is ignored for that call

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** A) The agent works normally without memory

**Explanation:** If no config with `thread_id` is passed, the agent still works but won't persist state. The checkpointer needs a thread_id to know where to save/load state, so without it, memory features won't function.
- **Why others are wrong:**
  - B) No error is raised; it's a silent degradation
  - C) There's no automatic default thread_id assignment
  - D) The checkpointer isn't ignored, it just has nothing to key on
</details>

---

### 35. What is the correct way to access state from within a tool function?
- [ ] A) Use a global variable
- [ ] B) Pass `runtime: ToolRuntime` as a parameter
- [ ] C) Access `self.state` directly
- [ ] D) Call `get_current_state()` function

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** B) Pass `runtime: ToolRuntime` as a parameter

**Explanation:** Tools access state, store, and config through the `ToolRuntime` object. Add `runtime: ToolRuntime` as a parameter, then use `runtime.state.get(...)`, `runtime.store.get(...)`, etc.
- **Why others are wrong:**
  - A) Global variables create tight coupling and testing issues
  - C) Tools are functions, not classes with `self`
  - D) No such function exists in LangChain
</details>

---

## Part 6: True/False (Continued)

### 36. The `temperature` parameter in `init_chat_model()` controls the creativity/randomness of model outputs.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** True

**Explanation:** The `temperature` parameter (typically 0.0-2.0) controls output randomness. Lower values (0.0) produce deterministic, focused outputs; higher values produce more creative, varied responses.
</details>

---

### 37. When using `.with_structured_output()`, you must manually parse the JSON response into a Pydantic object.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** False

**Explanation:** `.with_structured_output(PydanticModel)` handles parsing automatically. The return value is already a Pydantic model instance, not a raw JSON string. You can immediately access typed attributes like `result.name`.
</details>

---

### 38. A tool's function name automatically becomes the tool's name in LangChain.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** True

**Explanation:** When you decorate a function with `@tool`, the function's name becomes the tool's identifier. For example, `def search_products(query: str)` creates a tool named "search_products".
</details>

---

### 39. You can use `runtime.store` to persist data across different conversations (cross-thread).
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** True

**Explanation:** The store is for long-term, cross-conversation memory (like user preferences saved across sessions). State is session-scoped, but store persists and can be accessed from any thread.
</details>

---

### 40. LangSmith API keys have the format prefix `lsv2_pt_`.
- [ ] True
- [ ] False

<details>
<summary><b>ðŸ”Ž Click for Solution</b></summary>

**Correct Answer:** True

**Explanation:** LangSmith API keys follow the format `lsv2_pt_xxxxxxxxxxxxxxxxxxxxxxxxxxxx`. If the key doesn't start with this prefix, it may be invalid or from an older version.
</details>

---

## Answer Key Summary

**Answer Distribution:**
| Answer | Count | Questions |
|--------|-------|-----------|
| A | 5 | 1, 19, 34, 15, 17 |
| B | 5 | 4, 20, 29, 32, 35 |
| C | 8 | 3, 5, 8, 9, 10, 26, 28, 30, 31 |
| D | 4 | 2, 6, 7, 27 |
| True | 7 | 15, 17, 18, 36, 38, 39, 40 |
| False | 6 | 11, 12, 13, 14, 16, 37 |
| Fill-in | 4 | 22, 23, 24, 25 |

I have generated the Practice Quiz with detailed explanations for Week 5: LangChain v1.0. Please review!
